# launcher configuration

Obviously, the launcher needs to be parameterized. For convenience, it sports a builder with a fluent API.

You'll find the builder also in the main repolet artifact, and its javadoc [here](javadoc:com.braintribe.devrock.repolet.launcher.builder.api.LauncherCfgBuilderContext).

A simple example - that sets up a read-only repolet - looks like this :

```
	{
		launcher = LauncherCfgBuilderContext.build()
				.repolet()
					.name("archiveA")							
					.filesystem()
						.filesystem( new File( initial, "remoteRepoA"))
					.close()
				.close()
			.done();				
	}

```

The basic building blocks are as follows:

### Repolet Context

a repolet context contains the information concerning a single repolet.

```
launcher = LauncherCfgBuilderContext.build()
    .repolet()
            ..
    .close()
 .done()
```

A repolet context has the following properties:

### name
This is simply then name of the repolet. It will be reflected by the maven-metadata suffixes in your local repository.

```
.repolet()
	.name("archiveA")
  ..
.close()
```


### serverIdentification
This is what the repolet should answer in the HttpHeader _Server_. This can be used to tell a consumer what actual implementation is running the repository, for instance to mimic artifactory, you setup like this:

```
.repolet()
		..
		.serverIdentification("artifactory")
		..
.close()
```

### changesUrl
The changes URL is the formerly called 'ravenhurst URL'. It simply declares under which URL the repolet should answer with Ravenhurst notifications.
mc-legacy will need to have this URL configured in its settings, mc-core will find that out on its own during _probing_, as the repolet reflects on this

```
.repolet()
		..
		.changesUrl("http://localhost:${port}/archiveA/rest/changes")
		..
.close()
```

Note that if there are no specific change data (see below, the section about  _changes_), the repolet will simply return a list of the files it contains - just as Ravenhurst would if no timestamp's sent.

### restApiUrl
This is the URL where the REST API data can be accessed. This is an implementation of artifactory's REST API. The format is returned as JSON (just as artifactory does).

However, there's a model for it:

```
com.braintribe.devrock:artifactory-api-model
```

While the URL itself can be as you want it, artifactory has its own logic, and of course, the repolet can be set up accordingly (as mc-core, when _probing_ tells it that the repository implementation is artifactory, has a specific expectation).

```
    .repolet()
        ..
		    .restApiUrl("http://localhost:${port}/api/storage/archiveA")
        ..
		.close()
```

Note that the launcher will automatically substitute _${port}_ with the port number as determined at launch time.

### changes
This is used to better control what a consumer sees. As described above, just declaring the _changesUrl_ will lead the that repolet will _always_ return a listing of its content. If you want to have more control over it, you need to use this feature.

```
.repolet()
	   ..
		.changesUrl("http://localhost:${port}/archiveA/rest/changes")
		.changes( dateCodec.decode(testDate1AsString), new File( root, "setup/remoteRepoA/rh.answer.1.txt"))
		.changes( dateCodec.decode(testDate2AsString), new File( root, "setup/remoteRepoA/rh.answer.2.txt"))
		.changes( dateCodec.decode(testDate3AsString), new File( root, "setup/remoteRepoA/rh.answer.3.txt"))
    ..
    .close()
```

This now tells the repolet to respect the timestamp sent by RH queries. The repolet will determine into which of the declared time-spans the timestamp received falls and returns the associated answer.

The format for the date in string format is

```
"yyyy-MM-dd'T'HH:mm:ss.SSSZ"
```

and used is our very own

```
com.braintribe.codec.string.DateCodec
```

### filesystem
Of course, the repolet needs content. You can tell the repolet which local file system it should use to satisfy the requests.

```
.repolet()
     ...
     .filesystem()
		   .filesystem( new File( root, "contents/remoteRepoA"))
		 .close()
      ...
	.close()
```

*While you could basically add more than one file system, it is current not supported, i.e. you can only have one (_active_) file system*

The launcher (and the repolet) expect a structure as on the real repository or - simply - as generated by the [repolet content generator](./generator.md).

### indexedFilesystem
the index file system is intended to allow changes in the inner structure of the data. Of course, you could upload data to get the content changed, but it's way easier to use the indexed file system.
Basic idea is that you associate a filesystem with a key. And you can tell the repolet to switch its content to the filesystem matching the key you sent.

The configuration looks like this:

```
.repolet()
      ...
      .changesUrl("http://localhost:${port}/archive/rest/changes")
      .indexedFilesystem()
        .initialIndex("one")
        .filesystem( "one", new File( root, "contents/remoteRepoA"))
        .filesystem( "two", new File( root, "contents/remoteRepoB"))
      .close()
	.close()				
```

The tag _initalIndex_ tell the repolet what the content is to be at startup.

You can tell the repolet to switch its content with sending a HttpGet request.
The URL looks like this, and don't forget the port.

```
/update?key=<your key>
```

So in real life, you'd construct it like this if you wanted it to switch to index 'two'

```
      LauncherCfg cfg = launcher.getLaunchedCfg();
      RepoletCfg rcfg = cfg.getRepoletCfgs().get(0);

      String key = "two";
      String url = "http://localhost:${port}/" + rcfg.getName() + "/update";
      url = url.replace("${port}", "" + cfg.getPort());

      try {
          CloseableHttpResponse response = getGetResponse( url + "?key=" + key);
          HttpEntity entity = response.getEntity();
          EntityUtils.consume(entity);
      } catch (IOException e) {
      	e.printStackTrace();
      	Assert.fail("exception thrown while switching :" + e.getMessage());
      }			
```

### descriptiveContent

The descriptive content replaces the filesystem context. Instead of specifying the folder where the contents of the repolet is found, you actually declare the content here. 

```
.repolet()
     ...
     .descriptiveContent()
        .descriptiveContent( content)
      .close()
      ...
    .close()
```

The content here is an assembly of the types from

```
com.braintribe.devrock:repolet-content-model
```

You can use the the [repolet generator](./generator.md) to generate the contents on the file, for instance like this:

```
RepoletContent content = RepositoryGenerations.unmarshallConfigurationFile(configurationFile);

```

You'll find more about that in the part about the generator.

### indexedDescriptiveContent

As with the filesystem-based repolet, you can declare multiple content that is then switched. 

```
    .repolet()
     ...
        .indexedDescriptiveContent()
            .initialIndex("one")
            .descriptiveContent( "one", contentForFirstStage)
            .descriptiveContent( "two", contentForSecondStage)
        .close()
      ...
    .close()
```



### uploadFilesystem
As the repolet also supports uploading files, but shouldn't 'pollute' the prepared content, it needs a special file system as target for the uploads.

```
.repolet()
  ...
  .filesystem()
    .filesystem( new File( initial, "remoteRepoA"))
  .close()
  .uploadFilesystem()
    .filesystem( new File( target, "remoteRepoA"))
  .close()
  ...
```


### uploadReturnValueOverrides
The repolet it will - if all goes well - return 200 for a successful upload. For testing purposes - especially with artifactory - it can be told to return any Integer for an upload. 

    .repolet()
        .uploadReturnValueOverrides()
            .code("c-1.0.2.pom", 409)
            .... 
        .close()
        ...
    .close()

This code - 409 - is what artifact tells you when a pom file is well-formed, but syntactically wrong. 500 would come back if a standalone-hash file is uploaded before the file the hash was for is uploaded. 

### hashes
The repolet is programmed to ALWAYS calculate the md5,sha1 and sha256 hashes, and to add these values to the header. While that is how a remote repository is supposed to work, the repolet however needs to be able to return differing hashes, so that the consumer's behavior can be tested when it comes to discrepancies in hashes - that's what the repolet is for.

So you can specify your own hash values (so the repolet will not calculate the hashes) and/or tell it not to include the values in the headers. In that case, a standard 'maven compatible' consumer would actually access the standalone files for the hashes - associated to the actual file by the added extension with the hash-key. 

If you are accessing one of these files (for a 'xyz-1.0.pom' that would be 'xyz-1.0.pom.md5' for the md5 hash), the repolet will produce the file on the fly.

```
.repolet()
     ...
     .descriptiveContent()
        .descriptiveContent( content)
      .close()
     .hashes("xyz-1.0.pom")
        .hash( "X-Checksum-Md5", "bla-md5-bla")
        .hash( "X-Checksum-Sha1", "bla-sha1-bla")
        .hash( "X-Checksum-Sha256", "bla-sha256-bla")
    .close()        
      ...
.close()
```

The definition above will always return the strings specified in the hashes rather than the actual values. If any of the three hashes used are not declared, their value will be missing. Note that you need to specify the header's name rather than the digest.


```
.repolet()
     ...
     .descriptiveContent()
        .descriptiveContent( content)
      .close()
     .hashes("xyz-1.0.pom")
        .noHeaderSupport()   
    .close()        
      ...
.close()
```

This definition will not add the hashes to the headers, but use the actual hashes when asked for the pertinent files. 

You can of course combine the definition, i.e. you can specify custom hashes AND have these not returned in the headers.